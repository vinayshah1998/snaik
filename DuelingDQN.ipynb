{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DuelingDQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA9v51Xg0c2A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "88d059b5-55f2-4828-a896-4c6d8bafd0ee"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 178kB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-1.9.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Mscfd50o3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "#from keras.layers.core import Dense, Dropout\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import add\n",
        "\n",
        "\n",
        "class DQNAgent(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reward = 0\n",
        "        self.gamma = 0.9\n",
        "        self.dataframe = pd.DataFrame()\n",
        "        self.short_memory = np.array([])\n",
        "        self.agent_target = 1\n",
        "        self.agent_predict = 0\n",
        "        self.learning_rate = 0.0005\n",
        "        self.feature_model = self.feature_network()\n",
        "        self.value_model = self.value_network()\n",
        "        self.advantage_model = self.advantage_network()\n",
        "        self.model = self.network()\n",
        "        #self.model = self.network(\"weights.hdf5\")\n",
        "        self.epsilon = 0\n",
        "        self.actual = []\n",
        "        self.memory = []\n",
        "\n",
        "    def get_state(self, game, player, food):\n",
        "\n",
        "        state = [\n",
        "            (player.x_change == 20 and player.y_change == 0 and ((list(map(add, player.position[-1], [20, 0])) in player.position) or\n",
        "            player.position[-1][0] + 20 >= (game.game_width - 20))) or (player.x_change == -20 and player.y_change == 0 and ((list(map(add, player.position[-1], [-20, 0])) in player.position) or\n",
        "            player.position[-1][0] - 20 < 20)) or (player.x_change == 0 and player.y_change == -20 and ((list(map(add, player.position[-1], [0, -20])) in player.position) or\n",
        "            player.position[-1][-1] - 20 < 20)) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add, player.position[-1], [0, 20])) in player.position) or\n",
        "            player.position[-1][-1] + 20 >= (game.game_height-20))),  # danger straight\n",
        "\n",
        "            (player.x_change == 0 and player.y_change == -20 and ((list(map(add,player.position[-1],[20, 0])) in player.position) or\n",
        "            player.position[ -1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],\n",
        "            [-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == -20 and player.y_change == 0 and ((list(map(\n",
        "            add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n",
        "            (list(map(add,player.position[-1],[0,20])) in player.position) or player.position[-1][\n",
        "             -1] + 20 >= (game.game_height-20))),  # danger right\n",
        "\n",
        "             (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],[20,0])) in player.position) or\n",
        "             player.position[-1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == -20 and ((list(map(\n",
        "             add, player.position[-1],[-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n",
        "            (list(map(add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (\n",
        "            player.x_change == -20 and player.y_change == 0 and ((list(map(add,player.position[-1],[0,20])) in player.position) or\n",
        "            player.position[-1][-1] + 20 >= (game.game_height-20))), #danger left\n",
        "\n",
        "\n",
        "            player.x_change == -20,  # move left\n",
        "            player.x_change == 20,  # move right\n",
        "            player.y_change == -20,  # move up\n",
        "            player.y_change == 20,  # move down\n",
        "            food.x_food < player.x,  # food left\n",
        "            food.x_food > player.x,  # food right\n",
        "            food.y_food < player.y,  # food up\n",
        "            food.y_food > player.y  # food down\n",
        "            ]\n",
        "\n",
        "        for i in range(len(state)):\n",
        "            if state[i]:\n",
        "                state[i]=1\n",
        "            else:\n",
        "                state[i]=0\n",
        "\n",
        "        return np.asarray(state)\n",
        "\n",
        "    def set_reward(self, player, crash):\n",
        "        self.reward = 0\n",
        "        if crash:\n",
        "            self.reward = -10\n",
        "            return self.reward\n",
        "        if player.eaten:\n",
        "            self.reward = 10\n",
        "        return self.reward\n",
        "\n",
        "    def network(self, weights=None):\n",
        "        state = Input(shape=(11,), name='state')\n",
        "        features = Dense(120, activation='relu')(state)\n",
        "        features = Dropout(0.15)(features)\n",
        "        features = Dense(120, activation='relu')(features)\n",
        "        features = Dropout(0.15)(features)\n",
        "\n",
        "        value = Dense(120, activation='relu')(features)\n",
        "        value = Dropout(0.15)(value)\n",
        "        value = Dense(1, activation='softmax',name='value')(value)\n",
        "\n",
        "        advantages = Dense(120, activation='relu')(features)\n",
        "        advantages = Dropout(0.15)(advantages)\n",
        "        advantages = Dense(3, activation='softmax', name='advantages')(advantages)\n",
        "\n",
        "        model = Model(inputs = state, outputs = [value, advantages])\n",
        "        opt = Adam(self.learning_rate)\n",
        "        model.compile(loss='mse', optimizer=opt)\n",
        "        return model\n",
        "\n",
        "    def feature_network(self, weights=None):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(output_dim=120, activation='relu', input_dim=11))\n",
        "        model.add(Dropout(0.15))\n",
        "        model.add(Dense(output_dim=120, activation='relu'))\n",
        "        model.add(Dropout(0.15))\n",
        "        opt = Adam(self.learning_rate)\n",
        "        model.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "        if weights:\n",
        "            model.load_weights(weights)\n",
        "        return model\n",
        "\n",
        "    def value_network(self, weights=None):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(output_dim=120, activation='relu', input_dim=120))\n",
        "        model.add(Dropout(0.15))\n",
        "        model.add(Dense(output_dim=1, activation='softmax'))\n",
        "        opt = Adam(self.learning_rate)\n",
        "        model.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "        if weights:\n",
        "            model.load_weights(weights)\n",
        "        return model       \n",
        "\n",
        "    def advantage_network(self, weights=None):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(output_dim=120, activation='relu', input_dim=120))\n",
        "        model.add(Dropout(0.15))\n",
        "        model.add(Dense(output_dim=3, activation='softmax'))\n",
        "        opt = Adam(self.learning_rate)\n",
        "        model.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "        if weights:\n",
        "            model.load_weights(weights)\n",
        "        return model    \n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay_new(self, memory):\n",
        "        if len(memory) > 1000:\n",
        "            minibatch = random.sample(memory, 1000)\n",
        "        else:\n",
        "            minibatch = memory\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                value_next, advantages_next = self.model.predict(next_state.reshape((1,11)))                                                \n",
        "                Q_next = value_next[0] + (advantages_next[0] - np.mean(advantages_next[0])) \n",
        "                action_index = np.argmax(Q_next)\n",
        "                target = reward + self.gamma * Q_next[action_index]\n",
        "            value, advantages = self.model.predict(state.reshape((1,11)))\n",
        "            target_f = value[0] + (advantages[0] - np.mean(advantages[0])) \n",
        "            target_f[np.argmax(action)] = target \n",
        "            self.model.fit({'state':np.array([state])}, {'value':value, 'advantages':target_f.reshape(1,3)}, epochs=1, verbose=0)\n",
        "\n",
        "    def train_short_memory(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            value_next, advantages_next = self.model.predict(next_state.reshape((1,11)))                                                \n",
        "            Q_next = value_next[0] + (advantages_next[0] - np.mean(advantages_next[0])) \n",
        "            action_index = np.argmax(Q_next)\n",
        "            target = reward + self.gamma * Q_next[action_index]\n",
        "        value, advantages = self.model.predict(state.reshape((1,11)))\n",
        "        target_f = value[0] + (advantages[0] - np.mean(advantages[0])) \n",
        "        target_f[np.argmax(action)] = target \n",
        "        self.model.fit({'state':np.array([state])}, {'value':value, 'advantages':target_f.reshape(1,3)}, epochs=1, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RloBkj530r1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2778d257-4178-4f0f-be51-1a3b8c709eb9"
      },
      "source": [
        "import pygame\n",
        "from random import randint\n",
        "#from DoubleDQN import DQNAgent\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Set options to activate or deactivate the game view, and its speed\n",
        "display_option = False\n",
        "speed = 0\n",
        "pygame.font.init()\n",
        "\n",
        "\n",
        "class Game:\n",
        "\n",
        "    def __init__(self, game_width, game_height):\n",
        "        pygame.display.set_caption('SnakeGen')\n",
        "        self.game_width = game_width\n",
        "        self.game_height = game_height\n",
        "        #self.gameDisplay = pygame.display.set_mode((game_width, game_height+60))\n",
        "        #self.bg = pygame.image.load(\"img/background.png\")\n",
        "        self.crash = False\n",
        "        self.player = Player(self)\n",
        "        self.food = Food()\n",
        "        self.score = 0\n",
        "\n",
        "\n",
        "class Player(object):\n",
        "\n",
        "    def __init__(self, game):\n",
        "        x = 0.45 * game.game_width\n",
        "        y = 0.5 * game.game_height\n",
        "        self.x = x - x % 20\n",
        "        self.y = y - y % 20\n",
        "        self.position = []\n",
        "        self.position.append([self.x, self.y])\n",
        "        self.food = 1\n",
        "        self.eaten = False\n",
        "        #self.image = pygame.image.load('img/snakeBody.png')\n",
        "        self.x_change = 20\n",
        "        self.y_change = 0\n",
        "\n",
        "    def update_position(self, x, y):\n",
        "        if self.position[-1][0] != x or self.position[-1][1] != y:\n",
        "            if self.food > 1:\n",
        "                for i in range(0, self.food - 1):\n",
        "                    self.position[i][0], self.position[i][1] = self.position[i + 1]\n",
        "            self.position[-1][0] = x\n",
        "            self.position[-1][1] = y\n",
        "\n",
        "    def do_move(self, move, x, y, game, food,agent):\n",
        "        move_array = [self.x_change, self.y_change]\n",
        "\n",
        "        if self.eaten:\n",
        "\n",
        "            self.position.append([self.x, self.y])\n",
        "            self.eaten = False\n",
        "            self.food = self.food + 1\n",
        "        if np.array_equal(move ,[1, 0, 0]):\n",
        "            move_array = self.x_change, self.y_change\n",
        "        elif np.array_equal(move,[0, 1, 0]) and self.y_change == 0:  # right - going horizontal\n",
        "            move_array = [0, self.x_change]\n",
        "        elif np.array_equal(move,[0, 1, 0]) and self.x_change == 0:  # right - going vertical\n",
        "            move_array = [-self.y_change, 0]\n",
        "        elif np.array_equal(move, [0, 0, 1]) and self.y_change == 0:  # left - going horizontal\n",
        "            move_array = [0, -self.x_change]\n",
        "        elif np.array_equal(move,[0, 0, 1]) and self.x_change == 0:  # left - going vertical\n",
        "            move_array = [self.y_change, 0]\n",
        "        self.x_change, self.y_change = move_array\n",
        "        self.x = x + self.x_change\n",
        "        self.y = y + self.y_change\n",
        "\n",
        "        if self.x < 20 or self.x > game.game_width-40 or self.y < 20 or self.y > game.game_height-40 or [self.x, self.y] in self.position:\n",
        "            game.crash = True\n",
        "        eat(self, food, game)\n",
        "\n",
        "        self.update_position(self.x, self.y)\n",
        "\n",
        "    def display_player(self, x, y, food, game):\n",
        "        self.position[-1][0] = x\n",
        "        self.position[-1][1] = y\n",
        "\n",
        "        if game.crash == False:\n",
        "            for i in range(food):\n",
        "                x_temp, y_temp = self.position[len(self.position) - 1 - i]\n",
        "                #game.gameDisplay.blit(self.image, (x_temp, y_temp))\n",
        "\n",
        "            update_screen()\n",
        "        else:\n",
        "            pygame.time.wait(300)\n",
        "\n",
        "\n",
        "class Food(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.x_food = 240\n",
        "        self.y_food = 200\n",
        "        #self.image = pygame.image.load('img/food2.png')\n",
        "\n",
        "    def food_coord(self, game, player):\n",
        "        x_rand = randint(20, game.game_width - 40)\n",
        "        self.x_food = x_rand - x_rand % 20\n",
        "        y_rand = randint(20, game.game_height - 40)\n",
        "        self.y_food = y_rand - y_rand % 20\n",
        "        if [self.x_food, self.y_food] not in player.position:\n",
        "            return self.x_food, self.y_food\n",
        "        else:\n",
        "            self.food_coord(game,player)\n",
        "\n",
        "    def display_food(self, x, y, game):\n",
        "        game.gameDisplay.blit(self.image, (x, y))\n",
        "        update_screen()\n",
        "\n",
        "\n",
        "def eat(player, food, game):\n",
        "    if player.x == food.x_food and player.y == food.y_food:\n",
        "        food.food_coord(game, player)\n",
        "        player.eaten = True\n",
        "        game.score = game.score + 1\n",
        "\n",
        "\n",
        "def get_record(score, record):\n",
        "        if score >= record:\n",
        "            return score\n",
        "        else:\n",
        "            return record\n",
        "\n",
        "\n",
        "def display_ui(game, score, record):\n",
        "    myfont = pygame.font.SysFont('Segoe UI', 20)\n",
        "    myfont_bold = pygame.font.SysFont('Segoe UI', 20, True)\n",
        "    text_score = myfont.render('SCORE: ', True, (0, 0, 0))\n",
        "    text_score_number = myfont.render(str(score), True, (0, 0, 0))\n",
        "    text_highest = myfont.render('HIGHEST SCORE: ', True, (0, 0, 0))\n",
        "    text_highest_number = myfont_bold.render(str(record), True, (0, 0, 0))\n",
        "    game.gameDisplay.blit(text_score, (45, 440))\n",
        "    game.gameDisplay.blit(text_score_number, (120, 440))\n",
        "    game.gameDisplay.blit(text_highest, (190, 440))\n",
        "    game.gameDisplay.blit(text_highest_number, (350, 440))\n",
        "    game.gameDisplay.blit(game.bg, (10, 10))\n",
        "\n",
        "\n",
        "def display(player, food, game, record):\n",
        "    game.gameDisplay.fill((255, 255, 255))\n",
        "    display_ui(game, game.score, record)\n",
        "    player.display_player(player.position[-1][0], player.position[-1][1], player.food, game)\n",
        "    food.display_food(food.x_food, food.y_food, game)\n",
        "\n",
        "\n",
        "def update_screen():\n",
        "    pygame.display.update()\n",
        "\n",
        "\n",
        "def initialize_game(player, game, food, agent):\n",
        "    state_init1 = agent.get_state(game, player, food)  # [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
        "    action = [1, 0, 0]\n",
        "    player.do_move(action, player.x, player.y, game, food, agent)\n",
        "    state_init2 = agent.get_state(game, player, food)\n",
        "    reward1 = agent.set_reward(player, game.crash)\n",
        "    agent.remember(state_init1, action, reward1, state_init2, game.crash)\n",
        "    agent.replay_new(agent.memory)\n",
        "\n",
        "\n",
        "def plot_seaborn(array_counter, array_score):\n",
        "    sns.set(color_codes=True)\n",
        "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_score])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
        "    ax.set(xlabel='games', ylabel='score')\n",
        "    plt.show()\n",
        "\n",
        "def run():\n",
        "    pygame.init()\n",
        "    agent = DQNAgent()\n",
        "    counter_games = 0\n",
        "    score_plot = []\n",
        "    counter_plot =[]\n",
        "    record = 0\n",
        "    while counter_games < 150:\n",
        "        # Initialize classes\n",
        "        game = Game(440, 440)\n",
        "        player1 = game.player\n",
        "        food1 = game.food\n",
        "\n",
        "        # Perform first move\n",
        "        initialize_game(player1, game, food1, agent)\n",
        "        if display_option:\n",
        "            display(player1, food1, game, record)\n",
        "\n",
        "        while not game.crash:\n",
        "            #agent.epsilon is set to give randomness to actions\n",
        "            agent.epsilon = 80 - counter_games\n",
        "            \n",
        "            #get old state\n",
        "            state_old = agent.get_state(game, player1, food1)\n",
        "            \n",
        "            #perform random actions based on agent.epsilon, or choose the action\n",
        "            if randint(0, 200) < agent.epsilon:\n",
        "                final_move = to_categorical(randint(0, 2), num_classes=3)\n",
        "            else:\n",
        "                value, advantages = agent.model.predict(state_old.reshape((1,11)))\n",
        "                prediction = value[0] + (advantages[0] - np.mean(advantages[0])) \n",
        "                final_move = to_categorical(np.argmax(prediction), num_classes=3)\n",
        "                \n",
        "            #perform new move and get new state\n",
        "            player1.do_move(final_move, player1.x, player1.y, game, food1, agent)\n",
        "            state_new = agent.get_state(game, player1, food1)\n",
        "            \n",
        "            #set treward for the new state\n",
        "            reward = agent.set_reward(player1, game.crash)\n",
        "            \n",
        "            #train short memory base on the new action and state\n",
        "            agent.train_short_memory(state_old, final_move, reward, state_new, game.crash)\n",
        "            \n",
        "            # store the new data into a long term memory\n",
        "            agent.remember(state_old, final_move, reward, state_new, game.crash)\n",
        "            record = get_record(game.score, record)\n",
        "            if display_option:\n",
        "                display(player1, food1, game, record)\n",
        "                pygame.time.wait(speed)\n",
        "        \n",
        "        agent.replay_new(agent.memory)\n",
        "        counter_games += 1\n",
        "        print('Game', counter_games, '      Score:', game.score)\n",
        "        score_plot.append(game.score)\n",
        "        counter_plot.append(counter_games)\n",
        "    plot_seaborn(counter_plot, score_plot)\n",
        "    agent.model.save_weights('weights.hdf5')\n",
        "    from google.colab import files\n",
        "    files.download(\"weights.hdf5\")\n",
        "\n",
        "\n",
        "\n",
        "run()\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:104: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=120)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:106: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=120)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:117: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=120, units=120)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:119: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=1)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:129: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=120, units=120)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:131: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=3)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Game 1       Score: 0\n",
            "Game 2       Score: 0\n",
            "Game 3       Score: 0\n",
            "Game 4       Score: 0\n",
            "Game 5       Score: 0\n",
            "Game 6       Score: 0\n",
            "Game 7       Score: 0\n",
            "Game 8       Score: 0\n",
            "Game 9       Score: 0\n",
            "Game 10       Score: 0\n",
            "Game 11       Score: 0\n",
            "Game 12       Score: 1\n",
            "Game 13       Score: 0\n",
            "Game 14       Score: 0\n",
            "Game 15       Score: 0\n",
            "Game 16       Score: 0\n",
            "Game 17       Score: 0\n",
            "Game 18       Score: 0\n",
            "Game 19       Score: 1\n",
            "Game 20       Score: 1\n",
            "Game 21       Score: 0\n",
            "Game 22       Score: 1\n",
            "Game 23       Score: 1\n",
            "Game 24       Score: 1\n",
            "Game 25       Score: 1\n",
            "Game 26       Score: 0\n",
            "Game 27       Score: 1\n",
            "Game 28       Score: 1\n",
            "Game 29       Score: 1\n",
            "Game 30       Score: 1\n",
            "Game 31       Score: 1\n",
            "Game 32       Score: 1\n",
            "Game 33       Score: 1\n",
            "Game 34       Score: 1\n",
            "Game 35       Score: 5\n",
            "Game 36       Score: 2\n",
            "Game 37       Score: 3\n",
            "Game 38       Score: 1\n",
            "Game 39       Score: 3\n",
            "Game 40       Score: 3\n",
            "Game 41       Score: 4\n",
            "Game 42       Score: 2\n",
            "Game 43       Score: 3\n",
            "Game 44       Score: 1\n",
            "Game 45       Score: 3\n",
            "Game 46       Score: 1\n",
            "Game 47       Score: 3\n",
            "Game 48       Score: 1\n",
            "Game 49       Score: 4\n",
            "Game 50       Score: 1\n",
            "Game 51       Score: 2\n",
            "Game 52       Score: 2\n",
            "Game 53       Score: 1\n",
            "Game 54       Score: 5\n",
            "Game 55       Score: 1\n",
            "Game 56       Score: 1\n",
            "Game 57       Score: 3\n",
            "Game 58       Score: 2\n",
            "Game 59       Score: 1\n",
            "Game 60       Score: 1\n",
            "Game 61       Score: 2\n",
            "Game 62       Score: 1\n",
            "Game 63       Score: 4\n",
            "Game 64       Score: 1\n",
            "Game 65       Score: 3\n",
            "Game 66       Score: 4\n",
            "Game 67       Score: 5\n",
            "Game 68       Score: 4\n",
            "Game 69       Score: 4\n",
            "Game 70       Score: 1\n",
            "Game 71       Score: 5\n",
            "Game 72       Score: 3\n",
            "Game 73       Score: 1\n",
            "Game 74       Score: 5\n",
            "Game 75       Score: 4\n",
            "Game 76       Score: 8\n",
            "Game 77       Score: 17\n",
            "Game 78       Score: 6\n",
            "Game 79       Score: 7\n",
            "Game 80       Score: 3\n",
            "Game 81       Score: 1\n",
            "Game 82       Score: 9\n",
            "Game 83       Score: 3\n",
            "Game 84       Score: 10\n",
            "Game 85       Score: 9\n",
            "Game 86       Score: 6\n",
            "Game 87       Score: 2\n",
            "Game 88       Score: 4\n",
            "Game 89       Score: 10\n",
            "Game 90       Score: 7\n",
            "Game 91       Score: 7\n",
            "Game 92       Score: 12\n",
            "Game 93       Score: 2\n",
            "Game 94       Score: 10\n",
            "Game 95       Score: 3\n",
            "Game 96       Score: 1\n",
            "Game 97       Score: 10\n",
            "Game 98       Score: 1\n",
            "Game 99       Score: 14\n",
            "Game 100       Score: 23\n",
            "Game 101       Score: 45\n",
            "Game 102       Score: 6\n",
            "Game 103       Score: 13\n",
            "Game 104       Score: 14\n",
            "Game 105       Score: 8\n",
            "Game 106       Score: 1\n",
            "Game 107       Score: 12\n",
            "Game 108       Score: 16\n",
            "Game 109       Score: 12\n",
            "Game 110       Score: 16\n",
            "Game 111       Score: 1\n",
            "Game 112       Score: 4\n",
            "Game 113       Score: 9\n",
            "Game 114       Score: 5\n",
            "Game 115       Score: 5\n",
            "Game 116       Score: 13\n",
            "Game 117       Score: 28\n",
            "Game 118       Score: 28\n",
            "Game 119       Score: 23\n",
            "Game 120       Score: 20\n",
            "Game 121       Score: 9\n",
            "Game 122       Score: 45\n",
            "Game 123       Score: 17\n",
            "Game 124       Score: 16\n",
            "Game 125       Score: 18\n",
            "Game 126       Score: 11\n",
            "Game 127       Score: 17\n",
            "Game 128       Score: 20\n",
            "Game 129       Score: 2\n",
            "Game 130       Score: 17\n",
            "Game 131       Score: 33\n",
            "Game 132       Score: 10\n",
            "Game 133       Score: 14\n",
            "Game 134       Score: 5\n",
            "Game 135       Score: 27\n",
            "Game 136       Score: 3\n",
            "Game 137       Score: 10\n",
            "Game 138       Score: 19\n",
            "Game 139       Score: 7\n",
            "Game 140       Score: 19\n",
            "Game 141       Score: 5\n",
            "Game 142       Score: 2\n",
            "Game 143       Score: 3\n",
            "Game 144       Score: 11\n",
            "Game 145       Score: 6\n",
            "Game 146       Score: 7\n",
            "Game 147       Score: 3\n",
            "Game 148       Score: 13\n",
            "Game 149       Score: 5\n",
            "Game 150       Score: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-1d54cd330a15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-1d54cd330a15>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0mplot_seaborn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSMQ76Hvznyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}