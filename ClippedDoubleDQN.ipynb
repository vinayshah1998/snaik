{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClippedDoubleDQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg2_ZZhKkOd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "32028d04-559c-4097-9214-5d91b108d9ef"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 7.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-1.9.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRHoCdlVkv_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import add\n",
        "\n",
        "\n",
        "class DQNAgent(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reward = 0\n",
        "        self.gamma = 0.9\n",
        "        self.dataframe = pd.DataFrame()\n",
        "        self.short_memory = np.array([])\n",
        "        self.agent_target = 1\n",
        "        self.agent_predict = 0\n",
        "        self.learning_rate = 0.0005\n",
        "        self.model_1 = self.network()\n",
        "        self.model_2 = self.network()\n",
        "        #self.model = self.network(\"weights.hdf5\")\n",
        "        self.epsilon = 0\n",
        "        self.update_counter = 0  # only update target network after this counter hits certain value\n",
        "        self.actual = []\n",
        "        self.memory = []\n",
        "\n",
        "    def get_state(self, game, player, food):\n",
        "\n",
        "        state = [\n",
        "            (player.x_change == 20 and player.y_change == 0 and ((list(map(add, player.position[-1], [20, 0])) in player.position) or\n",
        "            player.position[-1][0] + 20 >= (game.game_width - 20))) or (player.x_change == -20 and player.y_change == 0 and ((list(map(add, player.position[-1], [-20, 0])) in player.position) or\n",
        "            player.position[-1][0] - 20 < 20)) or (player.x_change == 0 and player.y_change == -20 and ((list(map(add, player.position[-1], [0, -20])) in player.position) or\n",
        "            player.position[-1][-1] - 20 < 20)) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add, player.position[-1], [0, 20])) in player.position) or\n",
        "            player.position[-1][-1] + 20 >= (game.game_height-20))),  # danger straight\n",
        "\n",
        "            (player.x_change == 0 and player.y_change == -20 and ((list(map(add,player.position[-1],[20, 0])) in player.position) or\n",
        "            player.position[ -1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],\n",
        "            [-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == -20 and player.y_change == 0 and ((list(map(\n",
        "            add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n",
        "            (list(map(add,player.position[-1],[0,20])) in player.position) or player.position[-1][\n",
        "             -1] + 20 >= (game.game_height-20))),  # danger right\n",
        "\n",
        "             (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],[20,0])) in player.position) or\n",
        "             player.position[-1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == -20 and ((list(map(\n",
        "             add, player.position[-1],[-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n",
        "            (list(map(add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (\n",
        "            player.x_change == -20 and player.y_change == 0 and ((list(map(add,player.position[-1],[0,20])) in player.position) or\n",
        "            player.position[-1][-1] + 20 >= (game.game_height-20))), #danger left\n",
        "\n",
        "\n",
        "            player.x_change == -20,  # move left\n",
        "            player.x_change == 20,  # move right\n",
        "            player.y_change == -20,  # move up\n",
        "            player.y_change == 20,  # move down\n",
        "            food.x_food < player.x,  # food left\n",
        "            food.x_food > player.x,  # food right\n",
        "            food.y_food < player.y,  # food up\n",
        "            food.y_food > player.y  # food down\n",
        "            ]\n",
        "\n",
        "        for i in range(len(state)):\n",
        "            if state[i]:\n",
        "                state[i]=1\n",
        "            else:\n",
        "                state[i]=0\n",
        "\n",
        "        return np.asarray(state)\n",
        "\n",
        "    def set_reward(self, player, crash):\n",
        "        self.reward = 0\n",
        "        if crash:\n",
        "            self.reward = -10\n",
        "            return self.reward\n",
        "        if player.eaten:\n",
        "            self.reward = 10\n",
        "        return self.reward\n",
        "\n",
        "    def network(self, weights=None):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(output_dim=120, activation='relu', input_dim=11))\n",
        "        model.add(Dropout(0.15))\n",
        "        model.add(Dense(output_dim=120, activation='relu'))\n",
        "        model.add(Dropout(0.15))\n",
        "        model.add(Dense(output_dim=120, activation='relu'))\n",
        "        model.add(Dropout(0.15))\n",
        "        model.add(Dense(output_dim=3, activation='softmax'))\n",
        "        opt = Adam(self.learning_rate)\n",
        "        model.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "        if weights:\n",
        "            model.load_weights(weights)\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay_new(self, memory):\n",
        "        if len(memory) > 1000:\n",
        "            minibatch = random.sample(memory, 1000)\n",
        "        else:\n",
        "            minibatch = memory\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target1 = reward\n",
        "            target2 = reward\n",
        "            if not done:\n",
        "\n",
        "                Q_next_1 = self.model_1.predict(next_state.reshape((1, 11)))[0]\n",
        "                action_index_1 = np.argmax(Q_next_1)\n",
        "            \n",
        "                Q_cur_1 = self.model_1.predict(next_state.reshape((1, 11)))[0][action_index_1]\n",
        "                Q_cur_2 = self.model_2.predict(next_state.reshape((1, 11)))[0][action_index_1]\n",
        "\n",
        "                target1 = reward + self.gamma * min(Q_cur_1, Q_cur_2)\n",
        "\n",
        "                Q_next_2 = self.model_2.predict(next_state.reshape((1, 11)))[0]\n",
        "                action_index_2 = np.argmax(Q_next_2)\n",
        "\n",
        "                Q_cur_1 = self.model_1.predict(next_state.reshape((1, 11)))[0][action_index_2]\n",
        "                Q_cur_2 = self.model_2.predict(next_state.reshape((1, 11)))[0][action_index_2]\n",
        "\n",
        "                target2 = reward + self.gamma * min(Q_cur_1, Q_cur_2)\n",
        "                \n",
        "                \n",
        "            # self.target_model = self.q_model\n",
        "            target_f1 = self.model_1.predict(state.reshape((1, 11)))\n",
        "            target_f1[0][np.argmax(action)] = target1\n",
        "            target_f2 = self.model_2.predict(state.reshape((1, 11)))\n",
        "            target_f2[0][np.argmax(action)] = target2\n",
        "            self.model_1.fit(state.reshape((1, 11)), target_f1, epochs=1, verbose=0)\n",
        "            self.model_2.fit(state.reshape((1, 11)), target_f2, epochs=1, verbose=0)\n",
        "\n",
        "    def train_short_memory(self, state, action, reward, next_state, done):\n",
        "        target1 = reward\n",
        "        target2 = reward\n",
        "        if not done:\n",
        "\n",
        "            Q_next_1 = self.model_1.predict(next_state.reshape((1, 11)))[0]\n",
        "            action_index_1 = np.argmax(Q_next_1)\n",
        "            \n",
        "            Q_cur_1 = self.model_1.predict(next_state.reshape((1, 11)))[0][action_index_1]\n",
        "            Q_cur_2 = self.model_2.predict(next_state.reshape((1, 11)))[0][action_index_1]\n",
        "\n",
        "            target1 = reward + self.gamma * min(Q_cur_1, Q_cur_2)\n",
        "\n",
        "            Q_next_2 = self.model_2.predict(next_state.reshape((1, 11)))[0]\n",
        "            action_index_2 = np.argmax(Q_next_2)\n",
        "\n",
        "            Q_cur_1 = self.model_1.predict(next_state.reshape((1, 11)))[0][action_index_2]\n",
        "            Q_cur_2 = self.model_2.predict(next_state.reshape((1, 11)))[0][action_index_2]\n",
        "\n",
        "            target2 = reward + self.gamma * min(Q_cur_1, Q_cur_2)\n",
        "\n",
        "        # self.target_model = self.q_model\n",
        "        target_f1 = self.model_1.predict(state.reshape((1, 11)))\n",
        "        target_f1[0][np.argmax(action)] = target1\n",
        "        target_f2 = self.model_2.predict(state.reshape((1, 11)))\n",
        "        target_f2[0][np.argmax(action)] = target2\n",
        "        self.model_1.fit(state.reshape((1, 11)), target_f1, epochs=1, verbose=0)\n",
        "        self.model_2.fit(state.reshape((1, 11)), target_f2, epochs=1, verbose=0)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHLtM5_9m69l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f9e4b31-ca25-4399-c250-e24f92c6f424"
      },
      "source": [
        "import pygame\n",
        "from random import randint\n",
        "#from DoubleDQN import DQNAgent\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Set options to activate or deactivate the game view, and its speed\n",
        "display_option = False\n",
        "speed = 0\n",
        "pygame.font.init()\n",
        "\n",
        "\n",
        "class Game:\n",
        "\n",
        "    def __init__(self, game_width, game_height):\n",
        "        pygame.display.set_caption('SnakeGen')\n",
        "        self.game_width = game_width\n",
        "        self.game_height = game_height\n",
        "        #self.gameDisplay = pygame.display.set_mode((game_width, game_height+60))\n",
        "        #self.bg = pygame.image.load(\"img/background.png\")\n",
        "        self.crash = False\n",
        "        self.player = Player(self)\n",
        "        self.food = Food()\n",
        "        self.score = 0\n",
        "\n",
        "\n",
        "class Player(object):\n",
        "\n",
        "    def __init__(self, game):\n",
        "        x = 0.45 * game.game_width\n",
        "        y = 0.5 * game.game_height\n",
        "        self.x = x - x % 20\n",
        "        self.y = y - y % 20\n",
        "        self.position = []\n",
        "        self.position.append([self.x, self.y])\n",
        "        self.food = 1\n",
        "        self.eaten = False\n",
        "        #self.image = pygame.image.load('img/snakeBody.png')\n",
        "        self.x_change = 20\n",
        "        self.y_change = 0\n",
        "\n",
        "    def update_position(self, x, y):\n",
        "        if self.position[-1][0] != x or self.position[-1][1] != y:\n",
        "            if self.food > 1:\n",
        "                for i in range(0, self.food - 1):\n",
        "                    self.position[i][0], self.position[i][1] = self.position[i + 1]\n",
        "            self.position[-1][0] = x\n",
        "            self.position[-1][1] = y\n",
        "\n",
        "    def do_move(self, move, x, y, game, food,agent):\n",
        "        move_array = [self.x_change, self.y_change]\n",
        "\n",
        "        if self.eaten:\n",
        "\n",
        "            self.position.append([self.x, self.y])\n",
        "            self.eaten = False\n",
        "            self.food = self.food + 1\n",
        "        if np.array_equal(move ,[1, 0, 0]):\n",
        "            move_array = self.x_change, self.y_change\n",
        "        elif np.array_equal(move,[0, 1, 0]) and self.y_change == 0:  # right - going horizontal\n",
        "            move_array = [0, self.x_change]\n",
        "        elif np.array_equal(move,[0, 1, 0]) and self.x_change == 0:  # right - going vertical\n",
        "            move_array = [-self.y_change, 0]\n",
        "        elif np.array_equal(move, [0, 0, 1]) and self.y_change == 0:  # left - going horizontal\n",
        "            move_array = [0, -self.x_change]\n",
        "        elif np.array_equal(move,[0, 0, 1]) and self.x_change == 0:  # left - going vertical\n",
        "            move_array = [self.y_change, 0]\n",
        "        self.x_change, self.y_change = move_array\n",
        "        self.x = x + self.x_change\n",
        "        self.y = y + self.y_change\n",
        "\n",
        "        if self.x < 20 or self.x > game.game_width-40 or self.y < 20 or self.y > game.game_height-40 or [self.x, self.y] in self.position:\n",
        "            game.crash = True\n",
        "        eat(self, food, game)\n",
        "\n",
        "        self.update_position(self.x, self.y)\n",
        "\n",
        "    def display_player(self, x, y, food, game):\n",
        "        self.position[-1][0] = x\n",
        "        self.position[-1][1] = y\n",
        "\n",
        "        if game.crash == False:\n",
        "            for i in range(food):\n",
        "                x_temp, y_temp = self.position[len(self.position) - 1 - i]\n",
        "                #game.gameDisplay.blit(self.image, (x_temp, y_temp))\n",
        "\n",
        "            update_screen()\n",
        "        else:\n",
        "            pygame.time.wait(300)\n",
        "\n",
        "\n",
        "class Food(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.x_food = 240\n",
        "        self.y_food = 200\n",
        "        #self.image = pygame.image.load('img/food2.png')\n",
        "\n",
        "    def food_coord(self, game, player):\n",
        "        x_rand = randint(20, game.game_width - 40)\n",
        "        self.x_food = x_rand - x_rand % 20\n",
        "        y_rand = randint(20, game.game_height - 40)\n",
        "        self.y_food = y_rand - y_rand % 20\n",
        "        if [self.x_food, self.y_food] not in player.position:\n",
        "            return self.x_food, self.y_food\n",
        "        else:\n",
        "            self.food_coord(game,player)\n",
        "\n",
        "    def display_food(self, x, y, game):\n",
        "        game.gameDisplay.blit(self.image, (x, y))\n",
        "        update_screen()\n",
        "\n",
        "\n",
        "def eat(player, food, game):\n",
        "    if player.x == food.x_food and player.y == food.y_food:\n",
        "        food.food_coord(game, player)\n",
        "        player.eaten = True\n",
        "        game.score = game.score + 1\n",
        "\n",
        "\n",
        "def get_record(score, record):\n",
        "        if score >= record:\n",
        "            return score\n",
        "        else:\n",
        "            return record\n",
        "\n",
        "\n",
        "def display_ui(game, score, record):\n",
        "    myfont = pygame.font.SysFont('Segoe UI', 20)\n",
        "    myfont_bold = pygame.font.SysFont('Segoe UI', 20, True)\n",
        "    text_score = myfont.render('SCORE: ', True, (0, 0, 0))\n",
        "    text_score_number = myfont.render(str(score), True, (0, 0, 0))\n",
        "    text_highest = myfont.render('HIGHEST SCORE: ', True, (0, 0, 0))\n",
        "    text_highest_number = myfont_bold.render(str(record), True, (0, 0, 0))\n",
        "    game.gameDisplay.blit(text_score, (45, 440))\n",
        "    game.gameDisplay.blit(text_score_number, (120, 440))\n",
        "    game.gameDisplay.blit(text_highest, (190, 440))\n",
        "    game.gameDisplay.blit(text_highest_number, (350, 440))\n",
        "    game.gameDisplay.blit(game.bg, (10, 10))\n",
        "\n",
        "\n",
        "def display(player, food, game, record):\n",
        "    game.gameDisplay.fill((255, 255, 255))\n",
        "    display_ui(game, game.score, record)\n",
        "    player.display_player(player.position[-1][0], player.position[-1][1], player.food, game)\n",
        "    food.display_food(food.x_food, food.y_food, game)\n",
        "\n",
        "\n",
        "def update_screen():\n",
        "    pygame.display.update()\n",
        "\n",
        "\n",
        "def initialize_game(player, game, food, agent):\n",
        "    state_init1 = agent.get_state(game, player, food)  # [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
        "    action = [1, 0, 0]\n",
        "    player.do_move(action, player.x, player.y, game, food, agent)\n",
        "    state_init2 = agent.get_state(game, player, food)\n",
        "    reward1 = agent.set_reward(player, game.crash)\n",
        "    agent.remember(state_init1, action, reward1, state_init2, game.crash)\n",
        "    agent.replay_new(agent.memory)\n",
        "\n",
        "\n",
        "def plot_seaborn(array_counter, array_score):\n",
        "    sns.set(color_codes=True)\n",
        "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_score])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
        "    ax.set(xlabel='games', ylabel='score')\n",
        "    plt.show()\n",
        "\n",
        "def run():\n",
        "    pygame.init()\n",
        "    agent = DQNAgent()\n",
        "    counter_games = 0\n",
        "    score_plot = []\n",
        "    counter_plot =[]\n",
        "    record = 0\n",
        "    while counter_games < 150:\n",
        "        # Initialize classes\n",
        "        game = Game(440, 440)\n",
        "        player1 = game.player\n",
        "        food1 = game.food\n",
        "\n",
        "        # Perform first move\n",
        "        initialize_game(player1, game, food1, agent)\n",
        "        if display_option:\n",
        "            display(player1, food1, game, record)\n",
        "\n",
        "        while not game.crash:\n",
        "            #agent.epsilon is set to give randomness to actions\n",
        "            agent.epsilon = 80 - counter_games\n",
        "            \n",
        "            #get old state\n",
        "            state_old = agent.get_state(game, player1, food1)\n",
        "            \n",
        "            #perform random actions based on agent.epsilon, or choose the action\n",
        "            if randint(0, 200) < agent.epsilon:\n",
        "                final_move = to_categorical(randint(0, 2), num_classes=3)\n",
        "            else:\n",
        "                # predict action based on the old state (randomly picked)\n",
        "                if randint(0,10) < 5:\n",
        "                  prediction = agent.model_1.predict(state_old.reshape((1,11)))\n",
        "                else:\n",
        "                  prediction = agent.model_2.predict(state_old.reshape((1,11)))\n",
        "                final_move = to_categorical(np.argmax(prediction[0]), num_classes=3)\n",
        "                \n",
        "            #perform new move and get new state\n",
        "            player1.do_move(final_move, player1.x, player1.y, game, food1, agent)\n",
        "            state_new = agent.get_state(game, player1, food1)\n",
        "            \n",
        "            #set treward for the new state\n",
        "            reward = agent.set_reward(player1, game.crash)\n",
        "            \n",
        "            #train short memory base on the new action and state\n",
        "            agent.train_short_memory(state_old, final_move, reward, state_new, game.crash)\n",
        "            \n",
        "            # store the new data into a long term memory\n",
        "            agent.remember(state_old, final_move, reward, state_new, game.crash)\n",
        "            record = get_record(game.score, record)\n",
        "            if display_option:\n",
        "                display(player1, food1, game, record)\n",
        "                pygame.time.wait(speed)\n",
        "        \n",
        "        agent.replay_new(agent.memory)\n",
        "        counter_games += 1\n",
        "        print('Game', counter_games, '      Score:', game.score)\n",
        "        score_plot.append(game.score)\n",
        "        counter_plot.append(counter_games)\n",
        "    agent.model_1.save_weights('weights_1.hdf5')\n",
        "    agent.model_2.save_weights('weights_2.hdf5')\n",
        "    from google.colab import files\n",
        "    files.download(\"weights_1.hdf5\")\n",
        "    files.download(\"weights_2.hdf5\")\n",
        "    plot_seaborn(counter_plot, score_plot)\n",
        "\n",
        "\n",
        "run()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:81: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=120)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:83: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=120)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=120)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=3)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Game 1       Score: 1\n",
            "Game 2       Score: 0\n",
            "Game 3       Score: 2\n",
            "Game 4       Score: 0\n",
            "Game 5       Score: 0\n",
            "Game 6       Score: 0\n",
            "Game 7       Score: 1\n",
            "Game 8       Score: 2\n",
            "Game 9       Score: 0\n",
            "Game 10       Score: 1\n",
            "Game 11       Score: 1\n",
            "Game 12       Score: 4\n",
            "Game 13       Score: 1\n",
            "Game 14       Score: 4\n",
            "Game 15       Score: 0\n",
            "Game 16       Score: 2\n",
            "Game 17       Score: 4\n",
            "Game 18       Score: 4\n",
            "Game 19       Score: 4\n",
            "Game 20       Score: 3\n",
            "Game 21       Score: 4\n",
            "Game 22       Score: 3\n",
            "Game 23       Score: 10\n",
            "Game 24       Score: 4\n",
            "Game 25       Score: 4\n",
            "Game 26       Score: 3\n",
            "Game 27       Score: 4\n",
            "Game 28       Score: 3\n",
            "Game 29       Score: 4\n",
            "Game 30       Score: 4\n",
            "Game 31       Score: 6\n",
            "Game 32       Score: 5\n",
            "Game 33       Score: 6\n",
            "Game 34       Score: 7\n",
            "Game 35       Score: 4\n",
            "Game 36       Score: 7\n",
            "Game 37       Score: 4\n",
            "Game 38       Score: 5\n",
            "Game 39       Score: 13\n",
            "Game 40       Score: 5\n",
            "Game 41       Score: 9\n",
            "Game 42       Score: 6\n",
            "Game 43       Score: 4\n",
            "Game 44       Score: 4\n",
            "Game 45       Score: 5\n",
            "Game 46       Score: 7\n",
            "Game 47       Score: 3\n",
            "Game 48       Score: 13\n",
            "Game 49       Score: 4\n",
            "Game 50       Score: 5\n",
            "Game 51       Score: 4\n",
            "Game 52       Score: 5\n",
            "Game 53       Score: 11\n",
            "Game 54       Score: 4\n",
            "Game 55       Score: 4\n",
            "Game 56       Score: 6\n",
            "Game 57       Score: 4\n",
            "Game 58       Score: 10\n",
            "Game 59       Score: 12\n",
            "Game 60       Score: 5\n",
            "Game 61       Score: 15\n",
            "Game 62       Score: 7\n",
            "Game 63       Score: 12\n",
            "Game 64       Score: 6\n",
            "Game 65       Score: 4\n",
            "Game 66       Score: 11\n",
            "Game 67       Score: 11\n",
            "Game 68       Score: 23\n",
            "Game 69       Score: 8\n",
            "Game 70       Score: 18\n",
            "Game 71       Score: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1g0fFs_o74Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}